{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b420306",
   "metadata": {},
   "source": [
    "# Using online job postings to improve data science resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809d9ab",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "I want to find out what are the skills of data scientist not represented in my resume draft.  \n",
    "In order to improve the **CV** and start applying for the jobs.\n",
    "There is folder with all the job listings in HTML format.  \n",
    "Goal is to extract common data science skills and compare these skiils to the resume.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*rsJsA9wsN2Y5-o7HsItJ2A.jpeg\"\n",
    "     width=\"500\"\n",
    "     height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b175e",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "To achieve the goal we will do the following:\n",
    "\n",
    "1. Parse out all the text from the HTML files.\n",
    "2. Learn how job skills are commonly described in online postings.\n",
    "3. Filter irrelevant postings.\n",
    "4. Cluster the job skills within the relevant postings and visualize.\n",
    "5. Compare the clustered skills and our resume content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c976647",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1ff2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc23b8",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e4f4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458 HTML files have been loaded\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "html_contents = []\n",
    "\n",
    "with zipfile.ZipFile(\"job_postings.zip\", \"r\") as z:\n",
    "    for name in sorted(z.namelist()):\n",
    "        if name.endswith(\".html\"):\n",
    "            with z.open(name) as f:\n",
    "                html_contents.append(f.read().decode(\"utf-8\"))\n",
    "\n",
    "print(f\"{len(html_contents)} HTML files have been loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb859e9",
   "metadata": {},
   "source": [
    "Let's parse and store the parsed results in **soup_objects** list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a9aeb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1458</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1364</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Data Scientist - New York, NY</td>\n",
       "      <td>Data Scientist - Beavercreek, OH\\nData Scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title  \\\n",
       "count                            1458   \n",
       "unique                           1364   \n",
       "top     Data Scientist - New York, NY   \n",
       "freq                               13   \n",
       "\n",
       "                                                     Body  \n",
       "count                                                1458  \n",
       "unique                                               1458  \n",
       "top     Data Scientist - Beavercreek, OH\\nData Scienti...  \n",
       "freq                                                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soup_objects = []\n",
    "for html in html_contents:\n",
    "    soup = bs(html)\n",
    "    assert soup.title is not None\n",
    "    assert soup.body is not None \n",
    "    soup_objects.append(soup)\n",
    "html_dict = {'Title':[], 'Body':[]}\n",
    "for soup in soup_objects:\n",
    "    title = soup.find('title').text\n",
    "    body = soup.find('body').text\n",
    "    html_dict['Title'].append(title)\n",
    "    html_dict['Body'].append(body)\n",
    "\n",
    "df_jobs = pd.DataFrame(html_dict)\n",
    "summary = df_jobs.describe()\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771db412",
   "metadata": {},
   "source": [
    "There are 1364 unique titles out of 1458, the remaining 94 titles are duplicates.  \n",
    "\n",
    "The most common title repeated 13 times.  \n",
    "\n",
    "All 1458 bodies are unique, so none of the job postings occur more than once, even if some postings share common title.  \n",
    "  \n",
    "Now let's explore HTML content in more detail, the aim is to determine how jobs skills are described in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cef2c186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head><title>Patient Care Assistant / PCA - Med/Surg (Fayette, AL) - Fayette, AL</title></head>\n",
       "<body><h2>Patient Care Assistant / PCA - Med/Surg (Fayette, AL) - Fayette, AL</h2>\n",
       "<h1 class=\"jobSectionHeader\"><b>Performance Planning</b></h1>\n",
       "<p>Provides personal care and support services under the supervision of a registered nurse that assists the patient in the achievement of physical and emotional comfort.</p>\n",
       "<h1 class=\"jobSectionHeader\"><b>\n",
       "Key Responsibilities/Essential Functions</b></h1>\n",
       "<ul><li>Provides all personal care services in accordance with the plan of treatment assigned by the registered nurse</li><li>Accurately documents care provided</li><li>Applies safety principles and proper body mechanics to the performance of specific techniques of personal and supportive care, such as ambulation of patients, transferring patients, assisting with normal range of motions and positioning</li><li>Participates in economical utilization of supplies and ensures that equipment and nursing units are maintained in a clean, safe manner</li><li>Routinely follows and adheres to all policies and procedures</li><li>Assists in performance improvement (PI) activities by serving on PI teams as warranted, assisting with PI measures and supporting and implementing changes necessary for improvement</li></ul><ul><li>Maintains performance, patient and employee satisfaction and financial standards as outlined in the performance evaluation</li><li>Performs compliance requirements as outlined in the Employee Handbook</li><li>Must adhere to the DCH Behavioral Standards including creating positive relationships with patients/families, coworkers, colleagues and with self</li><li>Requires use of electronic mail, time and attendance software, learning management software and intranet</li><li>Must adhere to all DCH Health System policies and procedures</li><li>All other duties as assigned</li></ul><h1 class=\"jobSectionHeader\"><b>Minimum Knowledge, Skills, Experience Required</b></h1>\n",
       "<p>High School diploma or equivalent required. Basic Life Support training within 150 days of employment. Ability to meet behavioral expectations and work well with a team. Basic computer skills necessary to document patient care data into the clinical documentation system (vital signs, intake and output, etc.) Must be able to read, write, speak and comprehend English.\n",
       "</p><h1 class=\"jobSectionHeader\"><b>Working Conditions</b></h1>\n",
       "<p>Exposed to all patient care elements; subject to stress and fatigue. Prolonged periods of standing and/or walking; frequent kneeling and stooping. Ability to lift 50 pounds. Ability to transport, turn and position patients weighing up to 300 pounds. Vision and hearing normal or corrected to normal; manual dexterity to perform patient care tasks.</p></body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore the skill desriptions\n",
    "assert len(set(html_contents)) == len(html_contents)\n",
    "display(HTML(html_contents[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e14f5",
   "metadata": {},
   "source": [
    "There are usually 2 subsections, 1 for Responsibilities and 1 for Qualifications.  \n",
    "  \n",
    "They are not that different, yet qualifications focus on tools and concepts, while responsibilities are covering actions to be performed on the job.  \n",
    "  \n",
    "Let's divide posting description into 2 parts:\n",
    "  \n",
    "  \n",
    "A. Initial job summary.  \n",
    "\n",
    "B. List of skills required to get a job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6787bc",
   "metadata": {},
   "source": [
    "Do these types of skills desriptions also appear in other job postings.  \n",
    "Let' extract the bullets from each of parsed HTML files. \n",
    "Bullet point is taged in \"li\" and appears as dot in the html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "729ddcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a list of bullet points from soup object by calling following function\n",
    "df_jobs['Bullets'] = [[bullet.text.strip() for bullet in soup.find_all('li') ]for soup in soup_objects]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafeaa5e",
   "metadata": {},
   "source": [
    "We created new col \"Bullets\" to store all the bullet points, however, it may also be possible that majority of the job postings simply don't contain any!   \n",
    "  \n",
    "Let's find out the percentages of postings that actually contain bulleted text.  \n",
    "  \n",
    "If it's too low it is worth changing the approach of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a2fda2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.53% of the postings contain bullet points \n"
     ]
    }
   ],
   "source": [
    "bulleted_post_count = 0\n",
    "for bullet_list in df_jobs.Bullets:\n",
    "    if bullet_list:\n",
    "        bulleted_post_count += 1\n",
    "percent_bulleted = 100 * bulleted_post_count / df_jobs.shape[0]\n",
    "print(f\"{percent_bulleted:.2f}% of the postings contain bullet points \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fb347",
   "metadata": {},
   "source": [
    "Okay, next step is to understand whether most of these bullets focus on skills.  \n",
    "For that matter let's print out top-ranked words in their text. \n",
    "For ranking we will use **Term Frequency Inverse Document Frequency**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06191b79",
   "metadata": {},
   "source": [
    "### **How TFIDF Created**\n",
    "TF-IDF is the product of two distinct metrics designed to balance local importance and global rarity:\n",
    "\n",
    "1.  **Term Frequency (TF):** Measures how frequently a word appears in a specific document.\n",
    "    *   *Core Idea:* If a word appears often, it is likely important to that documentâ€™s topic.\n",
    "    *   *Formula:* $\\text{TF}(t, d) = \\frac{\\text{Count of term } t \\text{ in document } d}{\\text{Total number of words in document } d}$.\n",
    "2.  **Inverse Document Frequency (IDF):** Measures how rare a word is across the entire corpus.\n",
    "    *   *Core Idea:* Common words (like \"the\" or \"is\") appear in many documents and provide little unique information, so they are penalized with a lower weight.\n",
    "    *   *Formula:* $\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)$.\n",
    "3.  **Final Score:** $\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f0ffa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Words  Summed TFIDF\n",
      "experience    878.030398\n",
      "      data    842.978780\n",
      "    skills    440.780236\n",
      "      work    371.684232\n",
      "   ability    370.969638\n"
     ]
    }
   ],
   "source": [
    "def rank_words(text_list):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfdidf_matrix = vectorizer.fit_transform(text_list).toarray()\n",
    "    df = pd.DataFrame({'Words': vectorizer.get_feature_names_out(),\n",
    "                       \"Summed TFIDF\": tfdidf_matrix.sum(axis=0)})\n",
    "    sorted_df = df.sort_values('Summed TFIDF', ascending=False)\n",
    "    return sorted_df\n",
    "all_bullets = []\n",
    "for bullet_list in df_jobs.Bullets:\n",
    "    all_bullets.extend(bullet_list)\n",
    "sorted_df = rank_words(all_bullets)\n",
    "print(sorted_df[:5].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d269289",
   "metadata": {},
   "source": [
    "Terms such as **skills** and **ability** appear among the top five bulleted words.\n",
    "Most likely the bullets correspond to individual job skills.  \n",
    "How do these bulleted words compare to the remaining words in each job posting?  \n",
    "  \n",
    "  \n",
    "We iterate over the body of each posting and delete any bulleted lists using Beautiful Soup *decompose* method. \n",
    "Then we extract the remaining body text and store it in a non_bullets list. \n",
    "Finally, we apply rank_words function that lists and display the top five non-bullet words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc56f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Words  Summed TFIDF\n",
      "      data     99.111312\n",
      "      team     39.175041\n",
      "      work     38.928948\n",
      "experience     36.820836\n",
      "  business     36.140488\n"
     ]
    }
   ],
   "source": [
    "non_bullets = []\n",
    "for soup in soup_objects:\n",
    "    body = soup.body\n",
    "    for tag in body.find_all('li'):\n",
    "        tag.decompose()\n",
    "    non_bullets.append(body.text)\n",
    "sorted_df = rank_words(non_bullets)\n",
    "print(sorted_df[:5].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6d288",
   "metadata": {},
   "source": [
    "The words **skills** and **ability** are no longer present in the ranked output. \n",
    "They have been replaced by the words **business** and **team**. Thus, non-bulleted text appears to be less skill oriented.  \n",
    "  \n",
    "Words *data*, *experience* and *work* top ranked words that are shared between bullets and non-bulleted.  \n",
    "Strangely words **science** and **scientist** are missing.  \n",
    "\n",
    "Logic question arises, do some posts pertain to data-driven jobs aren't directly *data science* jobs? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb362863",
   "metadata": {},
   "source": [
    "Let's iterate over all the titles across all jobs and checking if each title mentions a data science position.  \n",
    "Then, we will measure the percentages of jobs where data science and data scientist are missing from the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04824551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.33% of the job postings titles do not mention a \n",
      "data science position. Below is a sample of such titles:\n",
      "\n",
      "Patient Care Assistant / PCA - Med/Surg (Fayette, AL) - Fayette, AL\n",
      "Data Manager / Analyst - Oakland, CA\n",
      "Scientific Programmer - Berkeley, CA\n",
      "JD Digits - AI Lab Research Intern - Mountain View, CA\n",
      "Operations and Technology Summer 2020 Internship-West Coast - Universal City, CA\n",
      "Data and Reporting Analyst - Olympia, WA 98501\n",
      "Senior Manager Advanced Analytics - Walmart Media Group - San Bruno, CA\n",
      "Data Specialist, Product Support Operations - Sunnyvale, CA\n",
      "Deep Learning Engineer - Westlake, TX\n",
      "Research Intern, 2020 - San Francisco, CA 94105\n"
     ]
    }
   ],
   "source": [
    "# we match our terms to the title using regular expressions.\n",
    "regex = r'Data Scien(?:ce|tist)'\n",
    "df_non_ds_jobs = df_jobs[~df_jobs.Title.str.contains(regex, case=False)]\n",
    "percent_non_ds = 100 * df_non_ds_jobs.shape[0] / df_jobs.shape[0]\n",
    "print(f'{percent_non_ds:.2f}% of the job postings titles do not mention a \\n'\n",
    "      f'data science position. Below is a sample of such titles:\\n')\n",
    "for title in df_non_ds_jobs.Title[:10]:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c22109",
   "metadata": {},
   "source": [
    "We can notice that many of these postings are still referring to data science, however worded differently.  \n",
    "For example, *data specialist* or *scientific programmer*. Moreover, some positions are research internships.  \n",
    "  \n",
    "  \n",
    "Which can be assured to be data-centric. Nevertheless, some positions are irrelevant such as managerial positions.  \n",
    "First posting contains word PCA, maybe during data collection crawler erroneously confused it with an PCA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6b317",
   "metadata": {},
   "source": [
    "The erroneous posting contains skills that we lack and have no interest in obtaining.  \n",
    "Thus, they should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6efd80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Provides all personal care services in accordance with the plan of treatment assigned by the registered nurse\n",
      "2: Accurately documents care provided\n",
      "3: Applies safety principles and proper body mechanics to the performance of specific techniques of personal and supportive care, such as ambulation of patients, transferring patients, assisting with normal range of motions and positioning\n",
      "4: Participates in economical utilization of supplies and ensures that equipment and nursing units are maintained in a clean, safe manner\n",
      "5: Routinely follows and adheres to all policies and procedures\n"
     ]
    }
   ],
   "source": [
    "# if not they will cause noise in our analysis\n",
    "bullets = df_non_ds_jobs.Bullets.iloc[0]\n",
    "for i, bullet in enumerate(bullets[:5],1):\n",
    "    print(f\"{i}: {bullet.strip()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b316cf0",
   "metadata": {},
   "source": [
    "These skills above are not of our interest. We need to delete them from the dataset.  \n",
    "The strategy is as follows: \n",
    "1. Obtain relevant job postings that partially match our existing skill set\n",
    "2. Examine which bullet points in these postings are missing from our existing skill set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09efcd",
   "metadata": {},
   "source": [
    "### Filtering jobs by relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3390fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience\n",
      "\n",
      "1. Developed probability simulations using NumPy.\n",
      "2. Assessed online ad-clicks for statistical significance using Permutation testing.\n",
      "3. Analyzed disease outbreaks using common clustering algorithms.\n",
      "\n",
      "Additional Skills\n",
      "\n",
      "1. Data visualization using Matplotlib.\n",
      "2. Statistical analysis using SciPy.\n",
      "3. Processing structured tables using Pandas.\n",
      "4. Executing K-Means clustering and DBSCAN clustering using Scikit-Learn.\n",
      "5. Extracting locations from text using GeonamesCache.\n",
      "6. Location analysis and visualization using GeonamesCache and Basemap.\n",
      "7. Dimensionality reduction with PCA and SVD, using Scikit-Learn.\n",
      "8. NLP analysis and text topic detection using Scikit-Learn.\n"
     ]
    }
   ],
   "source": [
    "# let's load our artificial resume\n",
    "resume = open('resume.txt','r').read()\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b9950",
   "metadata": {},
   "source": [
    "In the same manner we can store the table of contents in a variable *table_of_contents* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba6ef173",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_contents = open('table_of_contents.txt','r').read()\n",
    "existing_list = resume + table_of_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d97b8",
   "metadata": {},
   "source": [
    "Task is to compute the text_similarity between each job posting and our existing skills.  \n",
    "In other words, we want to compute all similarities between *df_jobs.Body* and *existing_skills*.  \n",
    "  \n",
    "Computation requires texts' vectorization of df_jobs.Body together with existing_skills.  \n",
    "That will ensure that vectors share the same vocabulary.  \n",
    "  \n",
    "Afterwards, we combine our job posts and our skill string into a single list of texts and vectorize these texts using scikit-learn\n",
    "**TfidfVectorizer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbc8533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df_jobs.Body.tolist() + [existing_list]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(text_list).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c130d2",
   "metadata": {},
   "source": [
    "The final matrix row (tfidf_matrix[-1]) corresponds to existing skillset and all other rows correspond to the job postings.  \n",
    "We can compute *cosine* similarities between the jobs postings and existing_skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c941e4e",
   "metadata": {},
   "source": [
    "### How cosine similiraty  calculated\n",
    "\n",
    "`TfidfVectorizer` normalizes vectors by default using **L2 normalization**.  \n",
    "This means each TF-IDF vector has unit length:\n",
    "\n",
    "$$\n",
    "\\|v\\| = 1\n",
    "$$\n",
    "\n",
    "The cosine similarity between two vectors **A** and **B** is defined as:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) =\n",
    "\\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$\n",
    "\n",
    "Since both vectors are normalized:\n",
    "\n",
    "$$\n",
    "\\|A\\| = \\|B\\| = 1\n",
    "$$\n",
    "\n",
    "The formula simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = A \\cdot B\n",
    "$$\n",
    "\n",
    "Therefore, when using normalized TF-IDF vectors, the **dot product is exactly the cosine similarity**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2548a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = tfidf_matrix[:-1] @ tfidf_matrix[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fd49e",
   "metadata": {},
   "source": [
    "Cosine similarity captures the text overlap between our existing skills and the posted jobs.  \n",
    "Jobs with greater overlap are more relevant and jobs with lesser overlap are less relevant.  \n",
    "  \n",
    "Thus, we can use cosine similarities to rank jobs by relevance.  \n",
    "First we will store it in *Relevance* column in **df_jobs** and then sort by it in descending order.  \n",
    "  \n",
    "Finally, we print the least relevant job titles and examine whehter they have anything to do with *data science*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "013e76b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst Internship (8 month minimum) - San Francisco, CA\n",
      "Leadership and Advocacy Coordinator - Oakland, CA 94607\n",
      "Finance Consultant - Audi Palo Alto - Palo Alto, CA\n",
      "RN - Hattiesburg, MS\n",
      "Configuration Management Specialist - Dahlgren, VA\n",
      "Deal Desk Analyst - Mountain View, CA\n",
      "Dev Ops Engineer AWS - Rockville, MD\n",
      "Web Development Teaching Assistant - UC Berkeley (Berkeley) - Berkeley, CA\n",
      "Scorekeeper - Oakland, CA 94612\n",
      "Direct Care - All Experience Levels (CNA, HHA, PCA Welcome) - Norwell, MA 02061\n",
      "Director of Marketing - Cambridge, MA\n",
      "Certified Strength and Conditioning Specialist - United States\n",
      "PCA - PCU Full Time - Festus, MO 63028\n",
      "Performance Improvement Consultant - Los Angeles, CA\n",
      "Patient Services Rep II - Oakland, CA\n",
      "Lab Researcher I - Richmond, CA\n",
      "Part-time instructor of Statistics for Data Science and Machine Learning - San Francisco, CA 94105\n",
      "Plant Engineering Specialist - San Pablo, CA\n",
      "Page Not Found - Indeed Mobile\n",
      "Director of Econometric Modeling - External Careers\n"
     ]
    }
   ],
   "source": [
    "df_jobs['Relevance'] = cosine_similarity\n",
    "sorted_df_jobs = df_jobs.sort_values(\"Relevance\", ascending=False)\n",
    "for title in sorted_df_jobs[-20:].Title:\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
